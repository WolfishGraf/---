# -*- coding: utf-8 -*-
"""Важность признаков-Claer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uuKCO_5IrdZPnjBsuWA1kF-JhP7qlgZZ

Данное задание основано на материалах лекций по логическим методам и направлено на знакомство c решающими деревьями (Decision Trees).

### Вы научитесь:
* обучать решающие деревья
* находить наиболее важные для них признаки
### Введение
Решающие деревья относятся к классу логических методов. Их основная идея состоит в объединении определенного количества простых решающих правил, благодаря чему итоговый алгоритм является интерпретируемым. Как следует из названия, решающее дерево представляет собой бинарное дерево, в котором каждой вершине сопоставлено некоторое правило вида "j-й признак имеет значение меньше b". В листьях этого дерева записаны числа-предсказания. Чтобы получить ответ, нужно стартовать из корня и делать переходы либо в левое, либо в правое поддерево в зависимости от того, выполняется правило из текущей вершины или нет.

Одна из особенностей решающих деревьев заключается в том, что они позволяют получать важности всех используемых признаков. Важность признака можно оценить на основе того, как сильно улучшился критерий качества благодаря использованию этого признака в вершинах дерева.

### Данные
В этом задании мы вновь рассмотрим данные о пассажирах Титаника. Будем решать на них задачу классификации, в которой по различным характеристикам пассажиров требуется предсказать, кто из них выжил после крушения корабля.

### Реализация в Scikit-Learn
В библиотеке scikit-learn решающие деревья реализованы в классах sklearn.tree.DecisionTreeСlassifier (для классификации) и sklearn.tree.DecisionTreeRegressor (для регрессии). Обучение модели производится с помощью функции fit.

Пример использования:

В этом задании вам также потребуется находить важность признаков. Это можно сделать, имея уже обученный классификатор:
"""

import numpy as np
from sklearn.tree import DecisionTreeClassifier
X = np.array([[-10, 0], [-5, 0], [10, 0], [10, 0]])
y = np.array([0, 1, 2, 3])
clf = DecisionTreeClassifier()
clf.fit(X, y)

importances = clf.feature_importances_
importances

"""Переменная importances будет содержать массив "важностей" признаков. Индекс в этом массиве соответствует индексу признака в данных.

Стоит обратить внимание, что данные могут содержать пропуски. Pandas хранит такие значения как nan (not a number). Для того, чтобы проверить, является ли число nan'ом, можно воспользоваться функцией np.isnan.

### Инструкция по выполнению
1. Загрузите выборку из файла train.csv с помощью пакета Pandas.
2. Оставьте в выборке четыре признака: класс пассажира (Pclass), цену билета (Fare), возраст пассажира (Age) и его пол (Sex).
3. Обратите внимание, что признак Sex имеет строковые значения.
4. Выделите целевую переменную — она записана в столбце Survived.
5. В данных есть пропущенные значения — например, для некоторых пассажиров неизвестен их возраст. Такие записи при чтении их в pandas принимают значение nan. Найдите все объекты, у которых есть пропущенные признаки, и удалите их из выборки.
6. Обучите решающее дерево с параметром random_state=241 и остальными параметрами по умолчанию (речь идет о параметрах конструктора DecisionTreeСlassifier).
7. Вычислите важности признаков и найдите два признака с наибольшей важностью. Их названия будут ответами для данной задачи (в качестве ответа укажите названия признаков через запятую или пробел, порядок не важен).
"""

import pandas as pd
data = pd.read_csv('train.csv', index_col='PassengerId')

data

df = pd.read_csv('train.csv',index_col='PassengerId').dropna() #загрузили и очистили пустые значения

df_Y= df[["Survived"]] # выделили целевую переменную 
df_X = df[["Pclass","Sex","Age","Fare"]]# выделили вторую группу значений

print(df_Y)
print(df_X)

df_X['Sex'] = df_X["Sex"].replace('male',1)
df_X['Sex'] = df_X["Sex"].replace('female',-1)
df_X['Sex']

from sklearn.tree import DecisionTreeClassifier
clf = DecisionTreeClassifier(random_state=241)
clf.fit(df_X,df_Y)

importances = clf.feature_importances_
importances